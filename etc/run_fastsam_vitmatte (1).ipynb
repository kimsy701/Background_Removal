{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ZO-77TOSdJgH83Un1n8mipZuG2gxt5q1","timestamp":1704274530720},{"file_id":"1M5HTj0Xu0w9Q_miEzeWplAf4SkpLAD0D","timestamp":1703599033429}],"gpuType":"T4","authorship_tag":"ABX9TyNEHW+DJuwW4qV/0OuMrB6x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","\n","print(torch.__version__)"],"metadata":{"id":"GcawYY6STdCQ","executionInfo":{"status":"ok","timestamp":1703832056205,"user_tz":-540,"elapsed":3647,"user":{"displayName":"Hyeyeon Kim","userId":"00338293452740477655"}},"outputId":"56d77e16-51e7-4fc6-b6fc-7da3cb885f30","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.1.0+cu121\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SsPFKtVBDtyj","executionInfo":{"status":"ok","timestamp":1703761774704,"user_tz":-540,"elapsed":2362,"user":{"displayName":"Hyeyeon Kim","userId":"00338293452740477655"}},"outputId":"ceeb864c-c954-4545-90e6-0db66f20728b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["#FAST-SAM\n","(SAM, SAM-HQ는 2024.01.03 현재 text prompt를 입력값으로 받지 않음, only point, only bounding box)\n","(그 앞에 grounding dino를 쓸 뿐)"],"metadata":{"id":"LxVgoZq6sBhW"}},{"cell_type":"code","source":["!git clone https://github.com/CASIA-IVA-Lab/FastSAM.git"],"metadata":{"id":"DQSy6OYztn8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://huggingface.co/spaces/An-619/FastSAM/resolve/main/weights/FastSAM.pt"],"metadata":{"id":"F8B2NFZltn-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -r FastSAM/requirements.txt\n","!pip install git+https://github.com/openai/CLIP.git"],"metadata":{"id":"sN-_7LrXtoAG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python FastSAM/Inference.py  --model_path FastSAM.pt --img_path ./images/dog.jpg --imgsz 1024"],"metadata":{"id":"R-QpIfgYtoB-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##한 이미지만 실행"],"metadata":{"id":"DHZA1qqzEcKt"}},{"cell_type":"code","source":["!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg"],"metadata":{"id":"ZS0INzvnt4mO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg  --text_prompt \"the yellow dog\""],"metadata":{"id":"rOoW3vQkzM7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gnTW16Aot4po"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 특정 input 폴더에 있는 이미지(png) 전부 실행"],"metadata":{"id":"qcTr7YUXEec1"}},{"cell_type":"code","source":[],"metadata":{"id":"dBut7CSCE4MB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#vitmatte\n"],"metadata":{"id":"H0ktbB5ua3VI"}},{"cell_type":"code","source":["# download codes and pre-trained weights\n","\n","import os\n","!git clone https://github.com/hustvl/ViTMatte.git\n","os.chdir('/content/ViTMatte')\n","\n","!pip install gdown\n","import gdown\n","url='https://drive.google.com/u/0/uc?id=12VKhSwE_miF9lWQQCgK7mv83rJIls3Xe'\n","file_name='ViTMatte_S_Com.pth'\n","gdown.download(url, file_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sMx5MPBra4iB","executionInfo":{"status":"ok","timestamp":1703759319636,"user_tz":-540,"elapsed":8834,"user":{"displayName":"Hyeyeon Kim","userId":"00338293452740477655"}},"outputId":"79cdfeb1-e471-4161-ebf9-a74664b000f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ViTMatte'...\n","remote: Enumerating objects: 162, done.\u001b[K\n","remote: Counting objects: 100% (55/55), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 162 (delta 42), reused 38 (delta 38), pack-reused 107\u001b[K\n","Receiving objects: 100% (162/162), 3.96 MiB | 20.38 MiB/s, done.\n","Resolving deltas: 100% (66/66), done.\n","Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Access denied with the following error:\n"]},{"output_type":"stream","name":"stderr","text":["\n"," \tToo many users have viewed or downloaded this file recently. Please\n","\ttry accessing the file again later. If the file you are trying to\n","\taccess is particularly large or is shared with many people, it may\n","\ttake up to 24 hours to be able to view or download the file. If you\n","\tstill can't access a file after 24 hours, contact your domain\n","\tadministrator. \n","\n","You may still be able to access the file from the browser:\n","\n","\t https://drive.google.com/u/0/uc?id=12VKhSwE_miF9lWQQCgK7mv83rJIls3Xe \n","\n"]}]},{"cell_type":"code","source":["# install packages\n","\n","!pip install fairscale timm\n","!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fw3yXJdXa5pJ","executionInfo":{"status":"ok","timestamp":1703759629390,"user_tz":-540,"elapsed":301332,"user":{"displayName":"Hyeyeon Kim","userId":"00338293452740477655"}},"outputId":"1dfc23d5-e102-4ff8-e0b5-85205c4067c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fairscale\n","  Downloading fairscale-0.4.13.tar.gz (266 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.12)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from fairscale) (2.1.0+cu121)\n","Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from fairscale) (1.23.5)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.4)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->fairscale) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->fairscale) (1.3.0)\n","Building wheels for collected packages: fairscale\n","  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332104 sha256=4b70894570797edf96185e0d7a86553d31e763173c9f9a51a19c5c0df95cde06\n","  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n","Successfully built fairscale\n","Installing collected packages: fairscale\n","Successfully installed fairscale-0.4.13\n","Collecting git+https://github.com/facebookresearch/detectron2.git\n","  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-bd7j1x3b\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-bd7j1x3b\n","  Resolved https://github.com/facebookresearch/detectron2.git to commit e9f7e2ba15abd7badcb05ef6f5076f06b36a9c5b\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.7)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.4.0)\n","Collecting yacs>=0.1.8 (from detectron2==0.6)\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n","Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.1)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.15.1)\n","Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n","  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n","  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hydra-core>=1.1 (from detectron2==0.6)\n","  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting black (from detectron2==0.6)\n","  Downloading black-23.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (23.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.23.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\n","Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.46.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n","Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n","  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n","Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.1.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n","Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.5.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.60.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.5.1)\n","Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.31.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n","Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n","  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=6119663 sha256=522d4d369ef8ed918afee7b7b8cadb01c77b07436e7fde5134b68c2823636263\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-6yzi9shl/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=8604bf3ac295f89261f2de1d7191ae3cdb887d34ac3a7e48538bb27b3cb36e20\n","  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=8502b5268acd2b29f7db681da10b3ed48d035521cb6b36a02fdef888b21540ea\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","Successfully built detectron2 fvcore antlr4-python3-runtime\n","Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n","Successfully installed antlr4-python3-runtime-4.9.3 black-23.12.1 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-2.8.2 yacs-0.1.8\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","#import gradio as gr\n","from PIL import Image\n","import torch\n","from torchvision.ops import box_convert\n","import sys\n","\n","#from detectron2.config import LazyConfig, instantiate\n","from detectron2.config.lazy import LazyConfig\n","from detectron2.config.instantiate import instantiate\n","from detectron2.checkpoint import DetectionCheckpointer"],"metadata":{"id":"PW_46AKKa5sf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vitmatte_models = {\n","\t'vit_b': '/content/drive/MyDrive/Matting-Anything/pretrained/ViTMatte_B_DIS.pth',\n","}\n","\n","vitmatte_config = {\n","\t'vit_b': '/content/drive/MyDrive/Matte-Anything/configs/matte_anything.py',\n","}"],"metadata":{"id":"WAJlz5e-a5vy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Optional, Tuple\n","\n","def generate_checkerboard_image(height, width, num_squares):\n","    num_squares_h = num_squares\n","    square_size_h = height // num_squares_h\n","    square_size_w = square_size_h\n","    num_squares_w = width // square_size_w\n","\n","\n","    new_height = num_squares_h * square_size_h\n","    new_width = num_squares_w * square_size_w\n","    image = np.zeros((new_height, new_width), dtype=np.uint8)\n","\n","    for i in range(num_squares_h):\n","        for j in range(num_squares_w):\n","            start_x = j * square_size_w\n","            start_y = i * square_size_h\n","            color = 255 if (i + j) % 2 == 0 else 200\n","            image[start_y:start_y + square_size_h, start_x:start_x + square_size_w] = color\n","\n","    image = cv2.resize(image, (width, height))\n","    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n","\n","    return image\n","\n","def init_vitmatte(model_type):\n","    \"\"\"\n","    Initialize the vitmatte with model_type in ['vit_s', 'vit_b']\n","    \"\"\"\n","\n","    import sys\n","    sys.path.append(\"/usr/local/lib/python3.10/dist-packages/detectron2/config\")\n","\n","    cfg = LazyConfig.load(vitmatte_config[model_type])\n","    vitmatte = instantiate(cfg.model)\n","    vitmatte.to(device)\n","    vitmatte.eval()\n","    DetectionCheckpointer(vitmatte).load(vitmatte_models[model_type])\n","\n","    return vitmatte\n","\n","def generate_trimap(mask, erode_kernel_size=10, dilate_kernel_size=10):\n","    erode_kernel = np.ones((erode_kernel_size, erode_kernel_size), np.uint8)\n","    dilate_kernel = np.ones((dilate_kernel_size, dilate_kernel_size), np.uint8)\n","    eroded = cv2.erode(mask, erode_kernel, iterations=5)\n","    dilated = cv2.dilate(mask, dilate_kernel, iterations=5)\n","    trimap = np.zeros_like(mask)\n","    trimap[dilated==255] = 128\n","    trimap[eroded==255] = 255\n","    return trimap\n","\n","def store_img(img):\n","    return img, []  # when new image is uploaded, `selected_points` should be empty\n","\n","def convert_pixels(gray_image, boxes):\n","    converted_image = np.copy(gray_image)\n","\n","    for box in boxes:\n","        x1, y1, x2, y2 = box\n","        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n","        converted_image[y1:y2, x1:x2][converted_image[y1:y2, x1:x2] == 1] = 0.5\n","\n","    return converted_image\n","\n","def covert_tracer_output_to_vitmatte_input(image_path): #image path: '~~/~~/~~.png'\n","    print(\"image_path\", image_path)\n","    # Load your background-removed image\n","    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n","\n","    # Assuming the alpha channel (if present) represents transparency (0 = removed, 255 = retained)\n","    if image.shape[2] == 4:  # Check if the image has an alpha channel\n","        alpha_channel = image[:, :, 3]\n","        mask = (alpha_channel == 0)  # Create a mask where removed part is True\n","\n","        # Invert the mask if necessary (to have True for removed part and False for foreground)\n","        mask = ~mask\n","\n","        # Convert the mask to a 1024x1024 list\n","        mask_list = mask.tolist()  # Convert the NumPy array to a list\n","        # Now mask_list contains the True/False values for removed/background parts\n","\n","        return mask_list\n","\n","import numpy as np\n","import torch\n","from PIL import Image\n","import torchvision.transforms.functional as F\n","\n","\n","#def cal_foreground(image_dir, alpha_dir):\n","def cal_foreground(image_dir, alpha):\n","    \"\"\"\n","    Calculate the foreground of the image.\n","    Input:\n","        image_dir: the directory of the image\n","        alpha_dir: the directory of the alpha matte\n","    Output:\n","        foreground: the foreground of the image, numpy array\n","    \"\"\"\n","    image = Image.open(image_dir).convert('RGB')\n","    #alpha = Image.open(alpha_dir).convert('L')\n","    alpha = alpha.convert('L')\n","    alpha = F.to_tensor(alpha).unsqueeze(0)\n","    image = F.to_tensor(image).unsqueeze(0)\n","    foreground = image * alpha + (1 - alpha)\n","    foreground = foreground.squeeze(0).permute(1, 2, 0).numpy()\n","\n","    return foreground\n","\n","\n","def cal_foreground2(image_dir, alpha_np):\n","    \"\"\"\n","    Calculate the foreground of the image with transparent background.\n","    Input:\n","        image_dir: the directory of the image\n","        alpha: the alpha mask as a PIL Image (1024, 1024)\n","    Output:\n","        foreground: the foreground of the image, PIL Image\n","    \"\"\"\n","    image = Image.open(image_dir).convert('RGBA')\n","\n","    # Create a new blank RGBA image with the same size as the original image\n","    foreground = Image.new('RGBA', image.size)\n","\n","    # Set alpha values based on the provided alpha mask\n","    for y in range(image.size[1]):\n","        for x in range(image.size[0]):\n","            r, g, b, a_original = image.getpixel((x, y))  # Get RGBA values from the original image\n","\n","            # Multiply alpha channel with the provided alpha mask\n","            a = int(a_original * alpha_np[y, x] / 255)  # Scale alpha values to [0, 255]\n","\n","            # Set the RGBA values for each pixel in the new image\n","            foreground.putpixel((x, y), (r, g, b, a))\n","\n","    return foreground\n","\n","\n","# common functions\n","\n","import os\n","from PIL import Image\n","from os.path import join as opj\n","from torchvision.transforms import functional as F\n","from detectron2.engine import default_argument_parser\n","from detectron2.config import LazyConfig, instantiate\n","from detectron2.checkpoint import DetectionCheckpointer\n","from google.colab.patches import cv2_imshow\n","import matplotlib.pyplot as plt\n","\n","def infer_one_image(model, input, save_dir=None):\n","    \"\"\"\n","    Infer the alpha matte of one image.\n","    Input:\n","        model: the trained model\n","        image: the input image\n","        trimap: the input trimap\n","    \"\"\"\n","    output = model(input)['phas'].flatten(0, 2)\n","    output = F.to_pil_image(output) #tensor to pil\n","\n","    #output.save(opj(save_dir))\n","    #output.save(save_dir)\n","\n","\n","    #return None\n","    return output\n","\n","def init_model(model, checkpoint, device):\n","    \"\"\"\n","    Initialize the model.\n","    Input:\n","        config: the config file of the model\n","        checkpoint: the checkpoint of the model\n","    \"\"\"\n","    assert model in ['vitmatte-s', 'vitmatte-b']\n","    if model == 'vitmatte-s':\n","        config = '/content/ViTMatte/configs/common/model.py'\n","        cfg = LazyConfig.load(config)\n","        model = instantiate(cfg.model)\n","        model.to(device)\n","        model.eval()\n","        DetectionCheckpointer(model).load(checkpoint)\n","    elif model == 'vitmatte-b':\n","        config = '/content/ViTMatte/configs/common/model.py'\n","        cfg = LazyConfig.load(config)\n","        cfg.model.backbone.embed_dim = 768\n","        cfg.model.backbone.num_heads = 12\n","        cfg.model.decoder.in_chans = 768\n","        model = instantiate(cfg.model)\n","        model.to(device)\n","        model.eval()\n","        DetectionCheckpointer(model).load(checkpoint)\n","    return model\n","\n","\n","def get_data(image_dir, trimap_dir):\n","#def get_data(image_dir, trimap):\n","    \"\"\"\n","    Get the data of one image.\n","    Input:\n","        image_dir: the directory of the image\n","        trimap_dir: the directory of the trimap\n","    \"\"\"\n","    image = Image.open(image_dir).convert('RGB')\n","    #image = Image.fromarray(image).convert('RGB')\n","    image = F.to_tensor(image).unsqueeze(0)\n","    trimap = Image.open(trimap_dir).convert('L')\n","    #trimap = Image.fromarray(trimap).convert('L')\n","    trimap = F.to_tensor(trimap).unsqueeze(0)\n","\n","    return {\n","        'image': image,\n","        'trimap': trimap\n","    }\n","\n","\n","\n","\n","\n","def merge_new_bg(image_dir, bg_dir, alpha_dir):\n","    \"\"\"\n","    Merge the alpha matte with a new background.\n","    Input:\n","        image_dir: the directory of the image\n","        bg_dir: the directory of the new background\n","        alpha_dir: the directory of the alpha matte\n","    \"\"\"\n","    image = Image.open(image_dir).convert('RGB')\n","    bg = Image.open(bg_dir).convert('RGB')\n","    alpha = Image.open(alpha_dir).convert('L')\n","    image = F.to_tensor(image)\n","    bg = F.to_tensor(bg)\n","    bg = F.resize(bg, image.shape[-2:])\n","    alpha = F.to_tensor(alpha)\n","    new_image = image * alpha + bg * (1 - alpha)\n","\n","    new_image = new_image.squeeze(0).permute(1, 2, 0).numpy()\n","    return new_image\n"],"metadata":{"id":"-7toYeaKa5zB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pHPPYF4Abteg","executionInfo":{"status":"ok","timestamp":1703599393828,"user_tz":-540,"elapsed":26086,"user":{"displayName":"Hyeyeon Kim","userId":"00338293452740477655"}},"outputId":"a59776be-fd30-423b-efe9-b79496b97ffa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["import time\n","\n","if torch.cuda.is_available():\n","    device = 'cuda'\n","else:\n","    device = 'cpu'\n","\n","#vitmatte_model = 'vit_b'\n","vitmatte_model = init_model(model='vitmatte-b', checkpoint='/content/gdrive/MyDrive/Matte-Anything/pretrained/ViTMatte_B_DIS.pth', device=device)\n","\n","colors = [(255, 0, 0), (0, 255, 0)]\n","markers = [1, 5] #marker type\n","\n","print('Initializing models... Please wait...')\n","\n","#predictor = init_segment_anything(sam_model)\n","#vitmatte = init_vitmatte(vitmatte_model)  # ->이거 나중에 내 깃헙 말고 다운받아서 가져오는 꼴로 하기\n","#grounding_dino =  dino_load_model(grounding_dino['config'], grounding_dino['weight'])#dino_load_model(grounding_dino['config'], grounding_dino['weight'])\n","\n","\n"," #   def run_vitmatte(vitmatte_input, selected_points, erode_kernel_size, dilate_kernel_size, fg_box_threshold, fg_text_threshold, fg_caption, tr_box_threshold, tr_text_threshold, tr_caption = \"glass, lens, crystal, diamond, bubble, bulb, web, grid\"):\n","def run_vitmatte(model, input_x,trimap_dir, masks, erode_kernel_size, dilate_kernel_size, fg_box_threshold, fg_text_threshold, fg_caption, tr_box_threshold, tr_text_threshold, tr_caption = \"glass, lens, crystal, diamond, bubble, bulb, web, grid\"):\n","    #set_image(input_x, \"RGB\")\n","\n","    # generate alpha matte\n","    torch.cuda.empty_cache()\n","    mask = masks.astype(np.uint8)*255\n","    trimap = generate_trimap(mask, erode_kernel_size, dilate_kernel_size).astype(np.float32)\n","    indices = np.where(trimap == 255)\n","    trimap[trimap==128] = 0.5\n","    trimap[trimap==255] = 1  # 잘 만들어짐 #array\n","\n","    cv2.imwrite(trimap_dir, trimap*255)\n","\n","    #input = get_data(input_x,trimap*255) #255해줌\n","    input = get_data(input_x, trimap_dir) #save_dir = where to save alpha(mask)\n","\n","    torch.cuda.empty_cache()\n","\n","    alpha = infer_one_image(model, input) #alpha: PIL Image\n","\n","    # get a green background\n","    image_array = cv2.imread(input_x)\n","    background = generate_checkerboard_image(image_array.shape[0], image_array.shape[1], 8)\n","\n","    # calculate foreground with alpha blending\n","    #foreground_alpha = input_x * np.expand_dims(alpha, axis=2).repeat(3,2)/255 + background * (1 - np.expand_dims(alpha, axis=2).repeat(3,2))/255\n","    foreground_alpha = cv2.cvtColor(image_array, cv2.COLOR_RGBA2RGB) * np.expand_dims(np.array(alpha), axis=2).repeat(3,2)/255 + background * (1 - np.expand_dims(np.array(alpha), axis=2).repeat(3,2))/255\n","\n","    #fg = cal_foreground(input_x, save_dir)\n","    fg=cal_foreground(input_x, alpha)\n","\n","    # Convert alpha to a numpy array\n","    alpha_np = alpha.convert('L')  # Convert to grayscale\n","    alpha_np = np.array(alpha_np)  # Convert to numpy array\n","\n","    a=time.time()\n","    fg2 = cal_foreground2(input_x, alpha_np)\n","    b=time.time()\n","    print(\"fg2 time per input\", b-a)\n","\n","    # calculate foreground with mask\n","    #foreground_mask = input_x * np.expand_dims(mask/255, axis=2).repeat(3,2)/255 + background * (1 - np.expand_dims(mask/255, axis=2).repeat(3,2))/255\n","    foreground_mask = cv2.cvtColor(image_array, cv2.COLOR_RGBA2RGB) * np.expand_dims(mask/255, axis=2).repeat(3,2)/255 + background * (1 - np.expand_dims(mask/255, axis=2).repeat(3,2))/255\n","\n","    #foreground_alpha[foreground_alpha>1] = 1\n","    foreground_mask[foreground_mask>1] = 1\n","\n","    # return img, mask_all\n","    trimap[trimap==1] == 0.999\n","\n","\n","    return  mask, alpha, foreground_mask, fg, fg2\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wrqlUAfvbSpR","executionInfo":{"status":"ok","timestamp":1703761532011,"user_tz":-540,"elapsed":2409,"user":{"displayName":"Hyeyeon Kim","userId":"00338293452740477655"}},"outputId":"fc043184-e3a3-458d-89fe-96e00d47c5b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing models... Please wait...\n"]}]},{"cell_type":"code","source":["\n","\n","\n","#사진 여러개에 대해서 실행하는 코드\n","\n","import os\n","import matplotlib.image as mpimg\n","\n","erode_kernel_size =10\n","dilate_kernel_size=10\n","fg_box_threshold=0.5\n","fg_text_threshold=0.5\n","#fg_caption=\"glass of water\" #for glass of water\n","fg_caption=\"cat\"\n","tr_box_threshold=0.5\n","tr_text_threshold=0.5\n","tr_caption= \"glass, lens, crystal, diamond, bubble, bulb, web, grid\"  #transparent 할것 같은 예시들\n","\n","# Define input and mask directories\n","input_dir = '/content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/input'\n","mask_dir = '/content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet' #수정\n","!mkdir /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet_vitmatte\n","final_dir = '/content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet_vitmatte'#수정\n","!mkdir /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet_vitmatte_transp\n","final_dir2 = '/content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet_vitmatte_transp'#수정\n","\n","# Get lists of files in both directories\n","input_files = os.listdir(input_dir)\n","mask_files = os.listdir(mask_dir)\n","\n","!mkdir /content/ViTMatte/trimap\n","\n","# Iterate through the files in the input directory\n","for input_file in input_files:\n","    # Ensure the file has the correct extension (e.g., PNG, JPG)\n","    if input_file.endswith('.png') or input_file.endswith('.jpg'):\n","        # Construct the full paths for input image and mask\n","        input_path = os.path.join(input_dir, input_file)\n","        mask_path = os.path.join(mask_dir, input_file)  # Assuming corresponding mask filenames are the same\n","        final_path = os.path.join(final_dir,input_file)\n","        final_path2 = os.path.join(final_dir2,input_file)\n","\n","        # Process the images\n","        # Your code to perform the operations using input_path and mask_path goes here\n","        # Example:\n","        mask_list = covert_tracer_output_to_vitmatte_input(mask_path)\n","        trimap_dir = '/content/ViTMatte/trimap/trimap.png'\n","        mask, alpha, foreground_mask, foreground_alpha, foreground_alpha2 = run_vitmatte(vitmatte_model, input_path, trimap_dir, np.array(mask_list).astype('uint8'), erode_kernel_size, dilate_kernel_size, fg_box_threshold, fg_text_threshold, fg_caption, tr_box_threshold, tr_text_threshold, tr_caption)\n","\n","        plt.imsave(final_path, np.array(foreground_alpha), format='png')    # Save as PNG format\n","        plt.imsave(final_path2, np.array(foreground_alpha2), format='png')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1l9sKGpwbSyW","executionInfo":{"status":"ok","timestamp":1703762206862,"user_tz":-540,"elapsed":310530,"user":{"displayName":"Hyeyeon Kim","userId":"00338293452740477655"}},"outputId":"e65e366c-4c3f-446b-c8d0-a51a3d0d7579"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet_vitmatte’: File exists\n","mkdir: cannot create directory ‘/content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet_vitmatte_transp’: File exists\n","mkdir: cannot create directory ‘/content/ViTMatte/trimap’: File exists\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/고양이-ai-generated-flamel (1) (1).png\n","fg2 time per input 4.536909341812134\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/강아지-ai-generated-flamel (1) (1).png\n","fg2 time per input 6.181604623794556\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/학생-ai-generated-flamel.png\n","fg2 time per input 4.633269309997559\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/햄버거-ai-generated-flamel.png\n","fg2 time per input 6.1103010177612305\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/커플-ai-generated-flamel.png\n","fg2 time per input 4.531714677810669\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/뉴욕거리-ai-generated-flamel.png\n","fg2 time per input 6.169228553771973\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/할로윈 거리를 다니는 학생들-ai-generated-flamel.png\n","fg2 time per input 4.505772113800049\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/고층빌딩이 있는 도시-ai-generated-flamel.png\n","fg2 time per input 5.472830533981323\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/서울 한복판에서 다니는 승용차-ai-generated-flamel.png\n","fg2 time per input 4.868745565414429\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/대치동 학원가-ai-generated-flamel.png\n","fg2 time per input 4.919082164764404\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/물이 반쯤 담긴 잔-ai-generated-flamel.png\n","fg2 time per input 5.453964710235596\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/와인이 담긴 잔이-ai-generated-flamel.png\n","fg2 time per input 4.525320529937744\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/술이 가득 차있는 소주잔-ai-generated-flamel.png\n","fg2 time per input 5.863605976104736\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/밖이 잘 보이는 창문-ai-generated-flamel.png\n","fg2 time per input 4.748843431472778\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/빨간색 방에 있는 1캐럿 다이아몬드-ai-generated-flamel.png\n","fg2 time per input 6.087883710861206\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/고양이-ai-generated-flamel (1).png\n","fg2 time per input 4.503708124160767\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/강아지-ai-generated-flamel (1).png\n","fg2 time per input 6.175078868865967\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/5세 남자아이-ai-generated-flamel.png\n","fg2 time per input 4.617511749267578\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/먹음직스러운 스테이크-ai-generated-flamel.png\n","fg2 time per input 6.027586936950684\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/길거리에 세워진 큰 차-ai-generated-flamel.png\n","fg2 time per input 4.487354516983032\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/카페에서 커피 한잔을 즐기는 여인-ai-generated-flamel.png\n","fg2 time per input 6.2170140743255615\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/풀밭에서 뛰어노는 말-ai-generated-flamel.png\n","fg2 time per input 4.653167009353638\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/빵을 먹으며 길거리를 다니는 남자-ai-generated-flamel.png\n","fg2 time per input 6.041407346725464\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/스파이더맨-ai-generated-flamel.png\n","fg2 time per input 4.551297426223755\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/크리스마스-ai-generated-flamel.png\n","fg2 time per input 6.123669862747192\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/a white disc on the floor.png\n","fg2 time per input 4.486367225646973\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/보라색 목도리를 한 귀여운 눈사람 캐릭터.png\n","fg2 time per input 6.388646125793457\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/an image of a Rubik's Cube.png\n","fg2 time per input 7.938173770904541\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/A Rubik's Cube, with Earth intricately drawn on each face, which is not solved yet, placed on a clean white background.png\n","fg2 time per input 5.400293827056885\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/경양식 돈까스-ai-generated-flamel.png\n","fg2 time per input 4.912602663040161\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/A robot standing front view, photo by wes anderson-ai-generated-flamel.png\n","fg2 time per input 5.390274286270142\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/보라색 목도리를 한 귀여운 눈사람-ai-generated-flamel.png\n","fg2 time per input 4.598059177398682\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/귀여운 보라색 요정-ai-generated-flamel.png\n","fg2 time per input 5.758012056350708\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/하얀 배경에 우측에 금색 장식들로 장식된 크리스마스 트리-ai-generated-flamel (5).png\n","fg2 time per input 4.65654993057251\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/크리스마스 케익-ai-generated-flamel.png\n","fg2 time per input 5.518369674682617\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/노란색 귀여운 소파-ai-generated-flamel.png\n","fg2 time per input 4.526390552520752\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/초록색 반짝이는 포장지로 싸이고 빨간색 끈으로 리본이 묶인 선물 박스-ai-generated-flamel.png\n","fg2 time per input 6.006963729858398\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/빈티지한 의자-ai-generated-flamel.png\n","fg2 time per input 4.572267770767212\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/a cartoon character holding a spoon with a scoop of ice cream in it.png\n","fg2 time per input 5.9944748878479\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/a cartoon character holding a spoon with a scoop of ice cream in it(1).png\n","fg2 time per input 4.54157018661499\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/a cartoon character holding a spoon with a scoop of ice cream in it(2).png\n","fg2 time per input 6.340076923370361\n","image_path /content/gdrive/MyDrive/스모어톡/프리렌서/배경제거/sam_test/output/inspyrenet/a cartoon character holding a spoon with a scoop of ice cream in it(3).png\n","fg2 time per input 4.49463963508606\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zwH6ujNnbSz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"doVDSE9wbTTC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lYli4aoFbTWz"},"execution_count":null,"outputs":[]}]}